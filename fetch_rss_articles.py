import feedparser
from newspaper import Article
import json
import os
from datetime import datetime

# å¯¾è±¡RSSãƒªã‚¹ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã›ã‚‹ï¼‰
RSS_FEEDS = [
    "https://www3.nhk.or.jp/rss/news/cat5.xml",        # NHK çµŒæ¸ˆ
    "https://www.nikkei.com/rss/economy.xml",          # æ—¥çµŒ çµŒæ¸ˆ
]

# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
SAVE_DIR = "data/news_articles"

def fetch_rss_articles():
    os.makedirs(SAVE_DIR, exist_ok=True)
    all_articles = []

    for url in RSS_FEEDS:
        print(f"\nğŸŒ RSSå–å¾—ä¸­: {url}")
        feed = feedparser.parse(url)
        for entry in feed.entries:
            article_url = entry.link
            print(f"ğŸ“° è¨˜äº‹URL: {article_url}")
            try:
                article = Article(article_url, language='ja')
                article.download()
                article.parse()
                article.nlp()

                record = {
                    "title": article.title,
                    "url": article_url,
                    "text": article.text,
                    "summary": article.summary,
                    "published": entry.get("published", ""),
                }
                all_articles.append(record)

            except Exception as e:
                print(f"âŒ å–å¾—å¤±æ•—: {article_url} â†’ {e}")

    if not all_articles:
        print("\nâš ï¸ å–å¾—ã§ããŸè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        today = datetime.now().strftime("%Y-%m-%d")
        file_path = os.path.join(SAVE_DIR, f"{today}_rss_articles.json")
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(all_articles, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… {len(all_articles)}ä»¶ã®è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ {file_path}")

    return all_articles

if __name__ == "__main__":
    print("ğŸš€ Plan Cï½œRSSãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹")
    fetch_rss_articles()
