import feedparser
from newspaper import Article
import json
import os
from datetime import datetime

# å¯¾è±¡RSSãƒªã‚¹ãƒˆ
RSS_FEEDS = [
    "https://www3.nhk.or.jp/rss/news/cat5.xml",
    "https://www.nikkei.com/rss/economy.xml",
]

# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
SAVE_DIR = "data/news_articles"
MEMORY_PATH = "memory.json"  # è¨˜æ†¶ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

def save_to_memory(articles, memory_path=MEMORY_PATH):
    """éå»ã®è¨˜æ†¶ã‚’ä¿å­˜ã™ã‚‹"""
    try:
        with open(memory_path, "r", encoding="utf-8") as f:
            memory = json.load(f)
    except FileNotFoundError:
        memory = []

    today_str = datetime.now().strftime("%Y-%m-%d")
    memory.append({"date": today_str, "articles": articles})

    with open(memory_path, "w", encoding="utf-8") as f:
        json.dump(memory, f, ensure_ascii=False, indent=2)

def fetch_rss_articles():
    os.makedirs(SAVE_DIR, exist_ok=True)
    all_articles = []

    for url in RSS_FEEDS:
        print(f"\nğŸŒ RSSå–å¾—ä¸­: {url}")
        feed = feedparser.parse(url)
        for entry in feed.entries:
            article_url = entry.link
            print(f"ğŸ“° è¨˜äº‹URL: {article_url}")
            try:
                article = Article(article_url, language='ja')
                article.download()
                article.parse()
                article.nlp()

                record = {
                    "title": article.title,
                    "url": article_url,
                    "text": article.text,
                    "summary": article.summary,
                    "published": entry.get("published", ""),
                }
                all_articles.append(record)

            except Exception as e:
                print(f"âŒ å–å¾—å¤±æ•—: {article_url} â†’ {e}")

    if not all_articles:
        print("\nâš ï¸ å–å¾—ã§ããŸè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        today = datetime.now().strftime("%Y-%m-%d")
        file_path = os.path.join(SAVE_DIR, f"{today}_rss_articles.json")
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(all_articles, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… {len(all_articles)}ä»¶ã®è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ {file_path}")

        # ğŸ§  è¨˜æ†¶ã«ä¿å­˜
        save_to_memory(all_articles)

    return all_articles

if __name__ == "__main__":
    print("ğŸš€ Plan Cï½œRSSãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹")
    fetch_rss_articles()
