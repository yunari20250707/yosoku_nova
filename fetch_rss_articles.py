import feedparser
from newspaper import Article
import json
import os
from datetime import datetime

# æ—¥ä»˜ã‚’å–å¾—
date_str = datetime.now().strftime("%Y-%m-%d")

# RSSãƒ•ã‚£ãƒ¼ãƒ‰ä¸€è¦§ï¼ˆå¿…è¦ãªã‚‰ã“ã“ã«ä»–åª’ä½“ã‚‚è¿½åŠ ï¼‰
RSS_FEEDS = {
    "nhk": "https://www3.nhk.or.jp/rss/news/cat5.xml",
    "nikkei": "https://www.nikkei.com/rss/newstopics.rdf",
}

# ä¿å­˜å‡¦ç†ï¼ˆç©ºãƒªã‚¹ãƒˆã§ã‚‚ä¿å­˜ï¼‰
def save_articles(articles, filepath):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ… ä¿å­˜å®Œäº†: {filepath}")
    # ğŸ” ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ãƒ­ã‚°
    if os.path.exists(filepath):
        print(f"ğŸ“¦ ç¢ºèª: ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ â†’ {filepath}")
    else:
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ â†’ {filepath}")

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def fetch_rss_articles():
    print("ğŸš€ Plan C | RSSãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹")
    for source, url in RSS_FEEDS.items():
        print(f"ğŸŒ RSSå–å¾—ä¸­ï¼š{url}")
        feed = feedparser.parse(url)
        articles = []

        for entry in feed.entries:
            try:
                article = Article(entry.link)
                article.download()
                article.parse()
                articles.append({
                    "title": article.title,
                    "text": article.text,
                    "url": entry.link,
                    "published": entry.get("published", ""),
                })
                print(f"ğŸ“„ è¨˜äº‹URL: {entry.link}")
            except Exception as e:
                print(f"âŒ å–å¾—å¤±æ•—: {entry.link} â†’ {e}")

        # ä¿å­˜ï¼ˆè¨˜äº‹ãŒ0ä»¶ã§ã‚‚ä¿å­˜ã™ã‚‹ï¼‰
        save_path = f"articles/{date_str}_{source}.json"
        save_articles(articles, save_path)

if __name__ == "__main__":
    fetch_rss_articles()